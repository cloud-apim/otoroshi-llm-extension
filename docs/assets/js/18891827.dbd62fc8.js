"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[1235],{3473:(e,i,r)=>{r.r(i),r.d(i,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>a});var n=r(4848),s=r(8453);r(9229);const o={sidebar_position:1},t="Overview",l={id:"overview",title:"Overview",description:"Otoroshi LLM Extension is set of Otoroshi plugins and resources to interact with LLMs, let's discover it !",source:"@site/docs/overview.mdx",sourceDirName:".",slug:"/overview",permalink:"/otoroshi-llm-extension/docs/overview",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",next:{title:"Install",permalink:"/otoroshi-llm-extension/docs/install"}},c={},a=[{value:"Supported LLM providers",id:"supported-llm-providers",level:2},{value:"MCP Servers",id:"mcp-servers",level:2},{value:"Cost Optimization",id:"cost-optimization",level:2},{value:"Ecological impacts",id:"ecological-impacts",level:2},{value:"Introduction video",id:"introduction-video",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"overview",children:"Overview"}),"\n",(0,n.jsx)(i.p,{children:"Otoroshi LLM Extension is set of Otoroshi plugins and resources to interact with LLMs, let's discover it !"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.strong,{children:"Connect, setup, secure and seamlessly manage LLM models using an Universal/OpenAI compatible API"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/llm-gateway/philosophy",children:(0,n.jsx)(i.strong,{children:"Unified interface"})}),": Simplify interactions and minimize integration hassles"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/llm-gateway/providers",children:(0,n.jsx)(i.strong,{children:"Use multiple providers"})}),": 10+ LLM providers supported right now, a lot more coming"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/llm-gateway/resilience",children:(0,n.jsx)(i.strong,{children:"Load balancing"})}),": Ensure optimal performance by distributing workloads across multiple providers"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"docs/llm-gateway/resilience#provider-fallback",children:(0,n.jsx)(i.strong,{children:"Fallbacks"})}),": Automatically switch LLMs during failures to deliver uninterrupted & accurate performance"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/llm-gateway/resilience",children:(0,n.jsx)(i.strong,{children:"Automatic retries"})}),": LLM APIs often have inexplicable failures. You can rescue a substantial number of your requests with our in-built automatic retries feature."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/cost-optimizations/semantic-cache",children:(0,n.jsx)(i.strong,{children:"Semantic cache"})}),": Speed up repeated queries, enhance response times, and reduce costs"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/cost-optimizations/quotas",children:(0,n.jsx)(i.strong,{children:"Custom quotas"})}),": Manage LLM tokens quotas per consumer and optimise costs"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/llm-gateway/secrets",children:(0,n.jsx)(i.strong,{children:"Key vault"})}),": securely store your LLM API keys in Otoroshi vault or any other secret vault supported by Otoroshi."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Observability and reporting"}),": every LLM request is audited with details about the consumer, the LLM provider and usage. All those audit events are exportable using multiple methods for further reporting"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Fine grained authorizations"}),": Use Otoroshi advanced fine grained authorizations capabilities to constrains model usage based on whatever you want: user identity, apikey, consumer metadata, request details, etc"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Prompt Fences"}),": Validate your prompts and prompts responses to avoid sensitive or personal informations leakage, irrelevant or unhelpful responses, gibberish content, etc"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.a,{href:"/docs/prompt-engineering/overview",children:(0,n.jsx)(i.strong,{children:"Prompt engineering"})}),": enhance your experience by providing contextual information to your prompts, storing them in a library for reusability, and using prompt templates for increased efficiency"]}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.a,{href:"/docs/guardrails/overview",children:(0,n.jsx)(i.strong,{children:"Guardrails"})})}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.a,{href:"/docs/cost-optimizations/cost-tracking",children:(0,n.jsx)(i.strong,{children:"Cost Tracking"})})}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.a,{href:"/docs/function-calling/mcp",children:(0,n.jsx)(i.strong,{children:"MCP (Model Context protocol)"})})}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.a,{href:"/docs/function-calling/mcp-connectors",children:(0,n.jsx)(i.strong,{children:"MCP Connectors"})})}),"\n",(0,n.jsx)(i.li,{children:(0,n.jsx)(i.a,{href:"/docs/function-calling/mcp-plugins",children:(0,n.jsx)(i.strong,{children:"MCP Server exposition"})})}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"supported-llm-providers",children:"Supported LLM providers"}),"\n",(0,n.jsxs)(i.p,{children:["All supported LLM providers are listed ",(0,n.jsx)(i.a,{href:"/docs/llm-gateway/providers",children:"here"})]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Anthropic"}),"\n",(0,n.jsx)(i.li,{children:"Azure OpenAI"}),"\n",(0,n.jsx)(i.li,{children:"Cloudflare"}),"\n",(0,n.jsx)(i.li,{children:"Cohere"}),"\n",(0,n.jsx)(i.li,{children:"Gemini"}),"\n",(0,n.jsx)(i.li,{children:"Groq"}),"\n",(0,n.jsx)(i.li,{children:"Huggingface \ud83c\uddeb\ud83c\uddf7 \ud83c\uddea\ud83c\uddfa"}),"\n",(0,n.jsx)(i.li,{children:"Mistral \ud83c\uddeb\ud83c\uddf7 \ud83c\uddea\ud83c\uddfa"}),"\n",(0,n.jsx)(i.li,{children:"Ollama (Local Models)"}),"\n",(0,n.jsx)(i.li,{children:"OpenAI"}),"\n",(0,n.jsx)(i.li,{children:"OVH AI Endpoints \ud83c\uddeb\ud83c\uddf7 \ud83c\uddea\ud83c\uddfa"}),"\n",(0,n.jsx)(i.li,{children:"Scaleway \ud83c\uddeb\ud83c\uddf7 \ud83c\uddea\ud83c\uddfa"}),"\n",(0,n.jsx)(i.li,{children:"X.ai"}),"\n",(0,n.jsx)(i.li,{children:"Deepseek"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"mcp-servers",children:"MCP Servers"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.a,{href:"/docs/function-calling/mcp",children:"MCP Servers"})}),"\n",(0,n.jsx)(i.h2,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.a,{href:"/docs/cost-optimizations",children:"Cost optimization"})}),"\n",(0,n.jsx)(i.h2,{id:"ecological-impacts",children:"Ecological impacts"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.a,{href:"/docs/reporting/ecological",children:"Ecological impacts"})}),"\n",(0,n.jsx)(i.h2,{id:"introduction-video",children:"Introduction video"}),"\n",(0,n.jsx)("iframe",{width:"956",height:"538",src:"https://www.youtube.com/embed/M8sA9xuE3gs?si=nSB80iS1Et9Q3eIT",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},9229:(e,i,r)=>{r(6540),r(4848)},8453:(e,i,r)=>{r.d(i,{R:()=>t,x:()=>l});var n=r(6540);const s={},o=n.createContext(s);function t(e){const i=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),n.createElement(o.Provider,{value:i},e.children)}}}]);
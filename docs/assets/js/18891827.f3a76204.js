"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[235],{3473:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var r=i(4848),s=i(8453);i(9229);const t={sidebar_position:1},o="Overview",l={id:"overview",title:"Overview",description:"Connect, setup, secure and seamlessly manage LLM models using an Universal/OpenAI compatible API",source:"@site/docs/overview.mdx",sourceDirName:".",slug:"/overview",permalink:"/otoroshi-llm-extension/docs/overview",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",next:{title:"Install",permalink:"/otoroshi-llm-extension/docs/install"}},a={},c=[{value:"Supported LLM providers",id:"supported-llm-providers",level:2},{value:"Introduction video",id:"introduction-video",level:2}];function d(e){const n={h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Connect, setup, secure and seamlessly manage LLM models using an Universal/OpenAI compatible API"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified interface"}),": Simplify interactions and minimize integration hassles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use multiple providers"}),": 10+ LLM providers supported right now, a lot more coming"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Load balancing"}),": Ensure optimal performance by distributing workloads across multiple providers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallbacks"}),": Automatically switch LLMs during failures to deliver uninterrupted & accurate performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic retries"}),": LLM APIs often have inexplicable failures. You can rescue a substantial number of your requests with our in-built automatic retries feature."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic cache"}),": Speed up repeated queries, enhance response times, and reduce costs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Custom quotas"}),": Manage LLM tokens quotas per consumer and optimise costs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key vault"}),": securely store your LLM API keys in Otoroshi vault or any other secret vault supported by Otoroshi."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Observability and reporting"}),": every LLM request is audited with details about the consumer, the LLM provider and usage. All those audit events are exportable using multiple methods for further reporting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine grained authorizations"}),": Use Otoroshi advanced fine grained authorizations capabilities to constrains model usage based on whatever you want: user identity, apikey, consumer metadata, request details, etc"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompt Fences"}),": Validate your prompts and prompts responses to avoid sensitive or personal informations leakage, irrelevant or unhelpful responses, gibberish content, etc"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompt engineering"}),": enhance your experience by providing contextual information to your prompts, storing them in a library for reusability, and using prompt templates for increased efficiency"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Otoroshi LLM Extension is set of Otoroshi plugins and resources to interact with LLMs, let's discover it"}),"\n",(0,r.jsx)(n.h2,{id:"supported-llm-providers",children:"Supported LLM providers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenAI"}),"\n",(0,r.jsx)(n.li,{children:"Azure OpenAI"}),"\n",(0,r.jsx)(n.li,{children:"Ollama"}),"\n",(0,r.jsx)(n.li,{children:"Mistral"}),"\n",(0,r.jsx)(n.li,{children:"Anthropic"}),"\n",(0,r.jsx)(n.li,{children:"Cohere"}),"\n",(0,r.jsx)(n.li,{children:"Gemini"}),"\n",(0,r.jsx)(n.li,{children:"Groq"}),"\n",(0,r.jsx)(n.li,{children:"Huggingface"}),"\n",(0,r.jsx)(n.li,{children:"OVH AI Endpoints"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-video",children:"Introduction video"}),"\n",(0,r.jsx)("iframe",{width:"956",height:"538",src:"https://www.youtube.com/embed/M8sA9xuE3gs?si=nSB80iS1Et9Q3eIT",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},9229:(e,n,i)=>{i(6540),i(4848)},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);
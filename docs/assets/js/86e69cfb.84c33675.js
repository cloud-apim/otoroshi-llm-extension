"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4580],{7013:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var a=t(4848),i=t(8453);t(9229);const s={sidebar_position:7},r="Observability",o={id:"llm-gateway/observability-reporting",title:"Observability",description:"Every interaction with a Large Language Model (LLM) generates crucial data that can be monitored, analyzed, and optimized.",source:"@site/docs/llm-gateway/observability-reporting.mdx",sourceDirName:"llm-gateway",slug:"/llm-gateway/observability-reporting",permalink:"/otoroshi-llm-extension/docs/llm-gateway/observability-reporting",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Resilience",permalink:"/otoroshi-llm-extension/docs/llm-gateway/resilience"},next:{title:"Costs optimizations",permalink:"/otoroshi-llm-extension/docs/category/costs-optimizations"}},l={},d=[{value:"\ud83d\udcca Key Metrics We Track",id:"-key-metrics-we-track",level:2},{value:"Using data exporter",id:"using-data-exporter",level:2},{value:"LLMUsageAudit event",id:"llmusageaudit-event",level:2},{value:"Dashboarding",id:"dashboarding",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"observability",children:"Observability"}),"\n",(0,a.jsxs)(e.p,{children:["Every interaction with a ",(0,a.jsx)(e.strong,{children:"Large Language Model (LLM)"})," generates crucial data that can be monitored, analyzed, and optimized."]}),"\n",(0,a.jsxs)(e.p,{children:["Our ",(0,a.jsx)(e.strong,{children:"LLM Gateway"})," provides ",(0,a.jsx)(e.strong,{children:"real-time tracking, security, and performance insights"}),", acting as a ",(0,a.jsx)(e.strong,{children:"centralized observability layer"})," to streamline LLM interactions."]}),"\n",(0,a.jsxs)(e.p,{children:["A ",(0,a.jsx)(e.strong,{children:"gateway"})," is the ",(0,a.jsx)(e.strong,{children:"ultimate solution"})," for managing, analyzing, and securing LLM traffic. By routing all requests through our LLM gateway, you gain:"]}),"\n",(0,a.jsx)(e.h2,{id:"-key-metrics-we-track",children:"\ud83d\udcca Key Metrics We Track"}),"\n",(0,a.jsxs)(e.p,{children:["Every LLM request logs ",(0,a.jsx)(e.strong,{children:"critical telemetry data"}),", including:"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"LLM Provider"})," \u2013 Identify the AI service in use (e.g., OpenAI, Anthropic, Google Gemini)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Model Version"})," \u2013 Track which model (GPT-4, Claude, Gemini) is processing requests."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Prompt Data"})," \u2013 Log input prompts to analyze patterns & improve outputs."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Response Data"})," \u2013 Capture AI-generated outputs for debugging & quality control."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Token Usage Metrics"})," \u2013 Measure input/output token consumption to optimize performance."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"User Identity"})," \u2013 Associate API usage with specific users for accountability."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"API Key Tracking"})," \u2013 Monitor & secure API access to prevent unauthorized use."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Authentication Tokens"})," \u2013 Ensure session integrity & compliance."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Connected User Sessions"})," \u2013 Identify active users interacting with the model."]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Unlike traditional monitoring tools, a ",(0,a.jsx)(e.strong,{children:"gateway"})," provides ",(0,a.jsx)(e.strong,{children:"full-stack observability"})," by capturing ",(0,a.jsx)(e.strong,{children:"every LLM request before it reaches the provider"}),"."]}),"\n",(0,a.jsxs)(e.p,{children:["This enables ",(0,a.jsx)(e.strong,{children:"granular control, cost efficiency, and real-time insights"}),"."]}),"\n",(0,a.jsx)(e.h2,{id:"using-data-exporter",children:"Using data exporter"}),"\n",(0,a.jsxs)(e.p,{children:["you can use ",(0,a.jsx)(e.a,{href:"https://maif.github.io/otoroshi/manual/entities/data-exporters.html",children:"otoroshi data exporters"})," to extract LLM usage informations and send it to anything you like"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{alt:"lb",src:t(4723).A+"",width:"1449",height:"1213"})}),"\n",(0,a.jsx)(e.p,{children:"just make sure to filter events on"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-json",children:'{\n  "include": [{\n    "audit": "LLMUsageAudit"\n  }],\n  "exclude": []\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"llmusageaudit-event",children:"LLMUsageAudit event"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-json",children:'{\n    "@id": "1904913942551986550",\n    "@timestamp": 1743001850151,\n    "@type": "AuditEvent",\n    "@product": "otoroshi",\n    "@serviceId": "",\n    "@service": "Otoroshi",\n    "@env": "dev",\n    "audit": "LLMUsageAudit",\n    "provider_kind": "openai",\n    "provider": "provider_f98538b5-6d59-426c-8127-cb583a9fa763",\n    "duration": 960,\n    "model": "gpt-4o-mini",\n    "rate_limit": {\n        "requests_limit": 10000,\n        "requests_remaining": 9998,\n        "tokens_limit": 200000,\n        "tokens_remaining": 199992\n    },\n    "usage": {\n        "prompt_tokens": 14,\n        "generation_tokens": 39,\n        "reasoning_tokens": 0\n    },\n    "error": null,\n    "consumed_using": "chat/completion/blocking",\n    "user": null,\n    "apikey": {\n        "_loc": {\n            "tenant": "default",\n            "teams": [\n                "default"\n            ]\n        },\n        "clientId": "the_client_id",\n        "clientName": "default-apikey",\n        "description": "the default apikey",\n        "authorizedGroup": "default",\n        "authorizedEntities": [\n            "group_default"\n        ],\n        "authorizations": [\n            {\n                "kind": "group",\n                "id": "default"\n            }\n        ],\n        "enabled": true,\n        "readOnly": false,\n        "allowClientIdOnly": false,\n        "throttlingQuota": 1,\n        "dailyQuota": 100000,\n        "monthlyQuota": 10000000,\n        "constrainedServicesOnly": false,\n        "restrictions": {\n            "enabled": false,\n            "allowLast": true,\n            "allowed": [],\n            "forbidden": [],\n            "notFound": []\n        },\n        "rotation": {\n            "enabled": false,\n            "rotationEvery": 744,\n            "gracePeriod": 168,\n            "nextSecret": null\n        },\n        "validUntil": null,\n        "tags": [],\n        "metadata": {\n            "updated_at": "2023-10-05T10:45:39.082+02:00",\n            "llm_tokens_limit": "201",\n            "llm_tokens_reset_after": "15000"\n        }\n    },\n    "route": {\n        "_loc": {\n            "tenant": "default",\n            "teams": [\n                "default"\n            ]\n        },\n        "id": "route_ec4670a82-2ada-485a-955a-bb710a1d237c",\n        "name": "demo-llm-events",\n        "description": "A new route",\n        "tags": [],\n        "metadata": {\n            "created_at": "2025-03-26T16:04:40.661+01:00",\n            "updated_at": "2025-03-26T16:05:09.563+01:00"\n        },\n        "enabled": true,\n        "debug_flow": false,\n        "export_reporting": false,\n        "capture": false,\n        "groups": [\n            "default"\n        ],\n        "bound_listeners": [],\n        "frontend": {\n            "domains": [\n                "demo-llm-events.oto.tools"\n            ],\n            "strip_path": true,\n            "exact": false,\n            "headers": {},\n            "query": {},\n            "methods": []\n        },\n        "backend": {\n            "targets": [\n                {\n                    "id": "target_1",\n                    "hostname": "request.otoroshi.io",\n                    "port": 443,\n                    "tls": true,\n                    "weight": 1,\n                    "backup": false,\n                    "predicate": {\n                        "type": "AlwaysMatch"\n                    },\n                    "protocol": "HTTP/1.1",\n                    "ip_address": null,\n                    "tls_config": {\n                        "certs": [],\n                        "trusted_certs": [],\n                        "enabled": false,\n                        "loose": false,\n                        "trust_all": false\n                    }\n                }\n            ],\n            "root": "/",\n            "rewrite": false,\n            "load_balancing": {\n                "type": "RoundRobin"\n            },\n            "client": {\n                "retries": 1,\n                "max_errors": 20,\n                "retry_initial_delay": 50,\n                "backoff_factor": 2,\n                "call_timeout": 30000,\n                "call_and_stream_timeout": 120000,\n                "connection_timeout": 10000,\n                "idle_timeout": 60000,\n                "global_timeout": 30000,\n                "sample_interval": 2000,\n                "proxy": {},\n                "custom_timeouts": [],\n                "cache_connection_settings": {\n                    "enabled": false,\n                    "queue_size": 2048\n                }\n            },\n            "health_check": {\n                "enabled": false,\n                "url": "",\n                "timeout": 5000,\n                "healthyStatuses": [],\n                "unhealthyStatuses": []\n            }\n        },\n        "backend_ref": null,\n        "plugins": [\n            {\n                "enabled": true,\n                "debug": false,\n                "plugin": "cp:otoroshi.next.plugins.OverrideHost",\n                "include": [],\n                "exclude": [],\n                "config": {},\n                "bound_listeners": [],\n                "plugin_index": {\n                    "transform_request": 0\n                }\n            },\n            {\n                "enabled": true,\n                "debug": false,\n                "plugin": "cp:otoroshi_plugins.com.cloud.apim.otoroshi.extensions.aigateway.plugins.OpenAiCompatProxy",\n                "include": [],\n                "exclude": [],\n                "config": {\n                    "refs": [\n                        "provider_f98538b5-6d59-426c-8127-cb583a9fa763"\n                    ]\n                },\n                "bound_listeners": [],\n                "plugin_index": {}\n            },\n            {\n                "enabled": true,\n                "debug": false,\n                "plugin": "cp:otoroshi.next.plugins.ApikeyCalls",\n                "include": [],\n                "exclude": [],\n                "config": {\n                    "extractors": {\n                        "basic": {\n                            "enabled": true,\n                            "header_name": null,\n                            "query_name": null\n                        },\n                        "custom_headers": {\n                            "enabled": true,\n                            "client_id_header_name": null,\n                            "client_secret_header_name": null\n                        },\n                        "client_id": {\n                            "enabled": true,\n                            "header_name": null,\n                            "query_name": null\n                        },\n                        "jwt": {\n                            "enabled": true,\n                            "secret_signed": true,\n                            "keypair_signed": true,\n                            "include_request_attrs": false,\n                            "max_jwt_lifespan_sec": null,\n                            "header_name": null,\n                            "query_name": null,\n                            "cookie_name": null\n                        }\n                    },\n                    "routing": {\n                        "enabled": false\n                    },\n                    "validate": true,\n                    "mandatory": true,\n                    "pass_with_user": false,\n                    "wipe_backend_request": true,\n                    "update_quotas": true\n                },\n                "bound_listeners": [],\n                "plugin_index": {\n                    "validate_access": 0,\n                    "transform_request": 1,\n                    "match_route": 0\n                }\n            }\n        ]\n    },\n    "input_prompt": [\n        {\n            "role": "user",\n            "content": "tell me a joke"\n        }\n    ],\n    "output": {\n        "generations": [\n            {\n                "message": {\n                    "role": "assistant",\n                    "content": "Why don\'t skeletons fight each other?\\n\\nThey don\'t have the guts! \ud83d\ude04"\n                }\n            }\n        ],\n        "metadata": {\n            "rate_limit": {\n                "requests_limit": 10000,\n                "requests_remaining": 9998,\n                "tokens_limit": 200000,\n                "tokens_remaining": 199992\n            },\n            "usage": {\n                "prompt_tokens": 14,\n                "generation_tokens": 39,\n                "reasoning_tokens": 0\n            }\n        }\n    },\n    "provider_details": {\n        "_loc": {\n            "tenant": "default",\n            "teams": [\n                "default"\n            ]\n        },\n        "id": "provider_f98538b5-6d59-426c-8127-cb583a9fa763",\n        "name": "OpenAI provider",\n        "description": "An OpenAI LLM api provider",\n        "metadata": {\n            "created_at": "2024-07-26T09:43:41.850+02:00",\n            "updated_at": "2025-02-28T12:06:21.250+01:00"\n        },\n        "tags": [],\n        "provider": "openai",\n        "connection": {\n            "base_url": "https://api.openai.com",\n            "token": "${vault://local/openai-token}",\n            "timeout": 30000\n        },\n        "options": {\n            "model": "gpt-4o-mini",\n            "frequency_penalty": null,\n            "logit_bias": null,\n            "logprobs": null,\n            "top_logprobs": null,\n            "max_tokens": "10000",\n            "n": 1,\n            "presence_penalty": null,\n            "response_format": null,\n            "seed": null,\n            "stop": null,\n            "stream": false,\n            "temperature": 1,\n            "top_p": 1,\n            "tools": null,\n            "tool_choice": null,\n            "user": null,\n            "wasm_tools": [],\n            "mcp_connectors": [],\n            "allow_config_override": false\n        },\n        "provider_fallback": null,\n        "context": {\n            "default": null,\n            "contexts": []\n        },\n        "models": {\n            "include": [],\n            "exclude": []\n        },\n        "guardrails": [],\n        "guardrails_fail_on_deny": false,\n        "cache": {\n            "strategy": "none",\n            "ttl": 86400000,\n            "score": 0.8\n        }\n    },\n    "user-agent-details": null,\n    "origin-details": null,\n    "instance-number": 0,\n    "instance-name": "dev",\n    "instance-zone": "local",\n    "instance-region": "local",\n    "instance-dc": "local",\n    "instance-provider": "local",\n    "instance-rack": "local",\n    "cluster-mode": "Leader",\n    "cluster-name": "otoroshi-leader-dev"\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"dashboarding",children:"Dashboarding"}),"\n",(0,a.jsxs)(e.p,{children:["We can use ",(0,a.jsx)(e.code,{children:"LLMUsageAudit"})," events to build a dashboard."]}),"\n",(0,a.jsx)(e.p,{children:"For example, we built a Grafana Dashboard to display some tokens consumption and other metrics."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:t(1772).A+"",width:"1607",height:"646"})})]})}function u(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},9229:(n,e,t)=>{t(6540),t(4848)},1772:(n,e,t)=>{t.d(e,{A:()=>a});const a=t.p+"assets/images/grafana-dashboard-4820f365fb1093fa65cc8215827bddb6.png"},4723:(n,e,t)=>{t.d(e,{A:()=>a});const a=t.p+"assets/images/llm-data-exporter-23dbb7cd06aa6f673550148aa27f3f12.png"},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(n){const e=a.useContext(s);return a.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);
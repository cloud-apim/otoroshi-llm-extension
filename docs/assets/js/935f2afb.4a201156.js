"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[8581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/overview","docId":"overview","unlisted":false},{"type":"link","label":"Install","href":"/otoroshi-llm-extension/docs/install","docId":"install","unlisted":false},{"type":"category","label":"LLM Gateway","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Philosophy","href":"/otoroshi-llm-extension/docs/llm-gateway/philosophy","docId":"llm-gateway/philosophy","unlisted":false},{"type":"link","label":"Supported LLM Providers","href":"/otoroshi-llm-extension/docs/llm-gateway/providers","docId":"llm-gateway/providers","unlisted":false},{"type":"link","label":"Setup a new LLM Provider","href":"/otoroshi-llm-extension/docs/llm-gateway/setup-provider","docId":"llm-gateway/setup-provider","unlisted":false},{"type":"link","label":"Expose your LLM Provider","href":"/otoroshi-llm-extension/docs/llm-gateway/expose","docId":"llm-gateway/expose","unlisted":false},{"type":"link","label":"Managing secrets","href":"/otoroshi-llm-extension/docs/llm-gateway/secrets","docId":"llm-gateway/secrets","unlisted":false},{"type":"link","label":"Resilience","href":"/otoroshi-llm-extension/docs/llm-gateway/resilience","docId":"llm-gateway/resilience","unlisted":false},{"type":"link","label":"Observability","href":"/otoroshi-llm-extension/docs/llm-gateway/observability-reporting","docId":"llm-gateway/observability-reporting","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/llm-gateway"},{"type":"category","label":"Costs optimizations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/cost-optimizations/overview","docId":"cost-optimizations/overview","unlisted":false},{"type":"link","label":"Managing tokens usage","href":"/otoroshi-llm-extension/docs/cost-optimizations/quotas","docId":"cost-optimizations/quotas","unlisted":false},{"type":"link","label":"\ud83d\udcb0 Costs tracking","href":"/otoroshi-llm-extension/docs/cost-optimizations/costs-tracking","docId":"cost-optimizations/costs-tracking","unlisted":false},{"type":"link","label":"Simple cache","href":"/otoroshi-llm-extension/docs/cost-optimizations/simple-cache","docId":"cost-optimizations/simple-cache","unlisted":false},{"type":"link","label":"Semantic cache","href":"/otoroshi-llm-extension/docs/cost-optimizations/semantic-cache","docId":"cost-optimizations/semantic-cache","unlisted":false},{"type":"link","label":"Reporting","href":"/otoroshi-llm-extension/docs/cost-optimizations/reporting","docId":"cost-optimizations/reporting","unlisted":false},{"type":"link","label":"\ud83c\udf3f Ecological impact","href":"/otoroshi-llm-extension/docs/cost-optimizations/ecological","docId":"cost-optimizations/ecological","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/costs-optimizations"},{"type":"category","label":"\ud83d\udee1\ufe0f Guardrails","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/guardrails/overview","docId":"guardrails/overview","unlisted":false},{"type":"link","label":"Auto Secrets Leakage","href":"/otoroshi-llm-extension/docs/guardrails/auto_secrets_leakage","docId":"guardrails/auto_secrets_leakage","unlisted":false},{"type":"link","label":"Characters count validation","href":"/otoroshi-llm-extension/docs/guardrails/characters","docId":"guardrails/characters","unlisted":false},{"type":"link","label":"Prompt contains guardrail","href":"/otoroshi-llm-extension/docs/guardrails/contains","docId":"guardrails/contains","unlisted":false},{"type":"link","label":"Prompt contains gender bias guardrail","href":"/otoroshi-llm-extension/docs/guardrails/gender_bias","docId":"guardrails/gender_bias","unlisted":false},{"type":"link","label":"Prompt contains gibberish guardrail","href":"/otoroshi-llm-extension/docs/guardrails/gibberish","docId":"guardrails/gibberish","unlisted":false},{"type":"link","label":"LLM guardrails","href":"/otoroshi-llm-extension/docs/guardrails/llm","docId":"guardrails/llm","unlisted":false},{"type":"link","label":"Prompt contains gibberish guardrail","href":"/otoroshi-llm-extension/docs/guardrails/moderation","docId":"guardrails/moderation","unlisted":false},{"type":"link","label":"Personal Health information","href":"/otoroshi-llm-extension/docs/guardrails/personal_health_information","docId":"guardrails/personal_health_information","unlisted":false},{"type":"link","label":"Personal information","href":"/otoroshi-llm-extension/docs/guardrails/pif","docId":"guardrails/pif","unlisted":false},{"type":"link","label":"Personal information","href":"/otoroshi-llm-extension/docs/guardrails/prompt_injection","docId":"guardrails/prompt_injection","unlisted":false},{"type":"link","label":"QuickJS","href":"/otoroshi-llm-extension/docs/guardrails/quickjs","docId":"guardrails/quickjs","unlisted":false},{"type":"link","label":"Racial Bias","href":"/otoroshi-llm-extension/docs/guardrails/racial_bias","docId":"guardrails/racial_bias","unlisted":false},{"type":"link","label":"Regex","href":"/otoroshi-llm-extension/docs/guardrails/regex","docId":"guardrails/regex","unlisted":false},{"type":"link","label":"Secrets Leakage","href":"/otoroshi-llm-extension/docs/guardrails/secrets_leakage","docId":"guardrails/secrets_leakage","unlisted":false},{"type":"link","label":"Semantic contains","href":"/otoroshi-llm-extension/docs/guardrails/semantic_contains","docId":"guardrails/semantic_contains","unlisted":false},{"type":"link","label":"Sentences Count","href":"/otoroshi-llm-extension/docs/guardrails/sentences","docId":"guardrails/sentences","unlisted":false},{"type":"link","label":"Toxic Language","href":"/otoroshi-llm-extension/docs/guardrails/toxic_language","docId":"guardrails/toxic_language","unlisted":false},{"type":"link","label":"WASM","href":"/otoroshi-llm-extension/docs/guardrails/wasm","docId":"guardrails/wasm","unlisted":false},{"type":"link","label":"Webhook","href":"/otoroshi-llm-extension/docs/guardrails/webhook","docId":"guardrails/webhook","unlisted":false},{"type":"link","label":"Words Count","href":"/otoroshi-llm-extension/docs/guardrails/words","docId":"guardrails/words","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/\ufe0f-guardrails"},{"type":"category","label":"Prompt Engineering","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/prompt-engineering/overview","docId":"prompt-engineering/overview","unlisted":false},{"type":"link","label":"Prompt templating","href":"/otoroshi-llm-extension/docs/prompt-engineering/prompt-templating","docId":"prompt-engineering/prompt-templating","unlisted":false},{"type":"link","label":"Prompt context","href":"/otoroshi-llm-extension/docs/prompt-engineering/prompt-context","docId":"prompt-engineering/prompt-context","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/prompt-engineering"},{"type":"category","label":"Function calling (tools)","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/function-calling/overview","docId":"function-calling/overview","unlisted":false},{"type":"link","label":"Wasm Functions","href":"/otoroshi-llm-extension/docs/function-calling/function-wasm-calling","docId":"function-calling/function-wasm-calling","unlisted":false},{"type":"link","label":"Http Functions","href":"/otoroshi-llm-extension/docs/function-calling/http-functions","docId":"function-calling/http-functions","unlisted":false},{"type":"link","label":"MCP (Model Context Protocol)","href":"/otoroshi-llm-extension/docs/function-calling/mcp","docId":"function-calling/mcp","unlisted":false},{"type":"link","label":"MCP Connectors","href":"/otoroshi-llm-extension/docs/function-calling/mcp-connectors","docId":"function-calling/mcp-connectors","unlisted":false},{"type":"link","label":"MCP server exposition","href":"/otoroshi-llm-extension/docs/function-calling/mcp-plugins","docId":"function-calling/mcp-plugins","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/function-calling-tools"},{"type":"category","label":"HTTP + LLM Plugins","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Request validation","href":"/otoroshi-llm-extension/docs/http-plugins/http-request-validation","docId":"http-plugins/http-request-validation","unlisted":false},{"type":"link","label":"HTTP request body modifier","href":"/otoroshi-llm-extension/docs/http-plugins/http-request-body-modifier","docId":"http-plugins/http-request-body-modifier","unlisted":false},{"type":"link","label":"HTTP response body modifier","href":"/otoroshi-llm-extension/docs/http-plugins/http-response-body-modifier","docId":"http-plugins/http-response-body-modifier","unlisted":false},{"type":"link","label":"HTTP response generator","href":"/otoroshi-llm-extension/docs/http-plugins/http-response-generator","docId":"http-plugins/http-response-generator","unlisted":false},{"type":"link","label":"Websocket message validation","href":"/otoroshi-llm-extension/docs/http-plugins/websocket-validation","docId":"http-plugins/websocket-validation","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/http--llm-plugins"},{"type":"category","label":"Embeddings","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/otoroshi-llm-extension/docs/embeddings/overview","docId":"embeddings/overview","unlisted":false},{"type":"link","label":"Supported Embedding providers","href":"/otoroshi-llm-extension/docs/embeddings/providers","docId":"embeddings/providers","unlisted":false},{"type":"link","label":"Setup a new Embedding model","href":"/otoroshi-llm-extension/docs/embeddings/embedding-models","docId":"embeddings/embedding-models","unlisted":false},{"type":"link","label":"Embedding plugins","href":"/otoroshi-llm-extension/docs/embeddings/plugins","docId":"embeddings/plugins","unlisted":false}],"href":"/otoroshi-llm-extension/docs/category/embeddings"},{"type":"link","label":"Try Otoroshi LLM Extension","href":"/otoroshi-llm-extension/docs/try-it","docId":"try-it","unlisted":false}]},"docs":{"cost-optimizations/costs-tracking":{"id":"cost-optimizations/costs-tracking","title":"\ud83d\udcb0 Costs tracking","description":"If you want to track the costs of your LLM Usage, you can enable it in the Otoroshi LLM Extension (it should be enabled by default)","sidebar":"tutorialSidebar"},"cost-optimizations/ecological":{"id":"cost-optimizations/ecological","title":"\ud83c\udf3f Ecological impact","description":"If you want to track the ecological impact of your LLM Usage, you can enable it in the Otoroshi LLM Extension (it should be enabled by default)","sidebar":"tutorialSidebar"},"cost-optimizations/overview":{"id":"cost-optimizations/overview","title":"Overview","description":"\u26a1 Cache: Fast and Efficient","sidebar":"tutorialSidebar"},"cost-optimizations/quotas":{"id":"cost-optimizations/quotas","title":"Managing tokens usage","description":"Our highly flexible quota management system ensures optimal resource allocation.","sidebar":"tutorialSidebar"},"cost-optimizations/reporting":{"id":"cost-optimizations/reporting","title":"Reporting","description":"You can use the audit events generated by your LLM usage to make some reporting dashboard and follow your metrics in near real-time.","sidebar":"tutorialSidebar"},"cost-optimizations/semantic-cache":{"id":"cost-optimizations/semantic-cache","title":"Semantic cache","description":"semantic cache uses an embedding datastore to find prompt with the same semantic","sidebar":"tutorialSidebar"},"cost-optimizations/simple-cache":{"id":"cost-optimizations/simple-cache","title":"Simple cache","description":"simple cache works on prompts word per word","sidebar":"tutorialSidebar"},"embeddings/embedding-models":{"id":"embeddings/embedding-models","title":"Setup a new Embedding model","description":"","sidebar":"tutorialSidebar"},"embeddings/overview":{"id":"embeddings/overview","title":"Overview","description":"","sidebar":"tutorialSidebar"},"embeddings/plugins":{"id":"embeddings/plugins","title":"Embedding plugins","description":"Otoroshi LLM extension provides a plugin called \'OpenAI compat. embedding\' to expose embedding models on a route","sidebar":"tutorialSidebar"},"embeddings/providers":{"id":"embeddings/providers","title":"Supported Embedding providers","description":"Here is a list of all embedding providers available in the Otoroshi LLM Extension :","sidebar":"tutorialSidebar"},"function-calling/function-wasm-calling":{"id":"function-calling/function-wasm-calling","title":"Wasm Functions","description":"\ud83e\udd16 What Are Tool Calls in LLMs?","sidebar":"tutorialSidebar"},"function-calling/http-functions":{"id":"function-calling/http-functions","title":"Http Functions","description":"","sidebar":"tutorialSidebar"},"function-calling/mcp":{"id":"function-calling/mcp","title":"MCP (Model Context Protocol)","description":"MCP is a set of rules that helps AI models communicate and understand context better when working with different apps, tools, or systems.","sidebar":"tutorialSidebar"},"function-calling/mcp-connectors":{"id":"function-calling/mcp-connectors","title":"MCP Connectors","description":"MCP connectors are tools to connect to MCP servers.","sidebar":"tutorialSidebar"},"function-calling/mcp-plugins":{"id":"function-calling/mcp-plugins","title":"MCP server exposition","description":"MCP Http Plugin","sidebar":"tutorialSidebar"},"function-calling/overview":{"id":"function-calling/overview","title":"Overview","description":"Otoroshi LLM Extension support classic function call provided by the consumer if the underlying provider supports it.","sidebar":"tutorialSidebar"},"guardrails/auto_secrets_leakage":{"id":"guardrails/auto_secrets_leakage","title":"Auto Secrets Leakage","description":"This Guardrail is a security measure that prevents the LLM from exposing sensitive information such as passwords, API keys, or confidential credentials, reducing the risk of data leaks. This safeguard can be applied before sending the prompt to the LLM (blocking requests that attempt to share secrets) and after generating a response (preventing accidental leaks).","sidebar":"tutorialSidebar"},"guardrails/characters":{"id":"guardrails/characters","title":"Characters count validation","description":"","sidebar":"tutorialSidebar"},"guardrails/contains":{"id":"guardrails/contains","title":"Prompt contains guardrail","description":"","sidebar":"tutorialSidebar"},"guardrails/gender_bias":{"id":"guardrails/gender_bias","title":"Prompt contains gender bias guardrail","description":"A mechanism that identifies and reduces biased language related to gender in user prompts, promoting fairness and inclusivity in AI-generated content. It can be applied before the LLM receives the request (blocking biased prompts) and after to filter or rephrase biased responses.","sidebar":"tutorialSidebar"},"guardrails/gibberish":{"id":"guardrails/gibberish","title":"Prompt contains gibberish guardrail","description":"This Guardrail acts like a filter that detects and manages inputs that are nonsensical, random, or meaningless, preventing the AI from generating irrelevant or low-quality responses.","sidebar":"tutorialSidebar"},"guardrails/llm":{"id":"guardrails/llm","title":"LLM guardrails","description":"","sidebar":"tutorialSidebar"},"guardrails/moderation":{"id":"guardrails/moderation","title":"Prompt contains gibberish guardrail","description":"Configuration","sidebar":"tutorialSidebar"},"guardrails/overview":{"id":"guardrails/overview","title":"Overview","description":"\ud83d\udea7 Enforcing Usage Limits","sidebar":"tutorialSidebar"},"guardrails/personal_health_information":{"id":"guardrails/personal_health_information","title":"Personal Health information","description":"It\'s a safeguard that ensures the LLM does not process, store, or share any personal health-related details, protecting user privacy and compliance with regulations.","sidebar":"tutorialSidebar"},"guardrails/pif":{"id":"guardrails/pif","title":"Personal information","description":"Configuration","sidebar":"tutorialSidebar"},"guardrails/prompt_injection":{"id":"guardrails/prompt_injection","title":"Personal information","description":"","sidebar":"tutorialSidebar"},"guardrails/quickjs":{"id":"guardrails/quickjs","title":"QuickJS","description":"","sidebar":"tutorialSidebar"},"guardrails/racial_bias":{"id":"guardrails/racial_bias","title":"Racial Bias","description":"","sidebar":"tutorialSidebar"},"guardrails/regex":{"id":"guardrails/regex","title":"Regex","description":"","sidebar":"tutorialSidebar"},"guardrails/secrets_leakage":{"id":"guardrails/secrets_leakage","title":"Secrets Leakage","description":"In the context of LLMs (Large Language Models) and AI, a \\"Secrets Leakage Guardrail\\" refers to a security mechanism designed to prevent AI models from exposing sensitive or confidential information. This can include API keys, passwords, proprietary business data, or personally identifiable information (PII).","sidebar":"tutorialSidebar"},"guardrails/semantic_contains":{"id":"guardrails/semantic_contains","title":"Semantic contains","description":"","sidebar":"tutorialSidebar"},"guardrails/sentences":{"id":"guardrails/sentences","title":"Sentences Count","description":"","sidebar":"tutorialSidebar"},"guardrails/toxic_language":{"id":"guardrails/toxic_language","title":"Toxic Language","description":"Configuration","sidebar":"tutorialSidebar"},"guardrails/wasm":{"id":"guardrails/wasm","title":"WASM","description":"","sidebar":"tutorialSidebar"},"guardrails/webhook":{"id":"guardrails/webhook","title":"Webhook","description":"","sidebar":"tutorialSidebar"},"guardrails/words":{"id":"guardrails/words","title":"Words Count","description":"","sidebar":"tutorialSidebar"},"http-plugins/http-request-body-modifier":{"id":"http-plugins/http-request-body-modifier","title":"HTTP request body modifier","description":"","sidebar":"tutorialSidebar"},"http-plugins/http-request-validation":{"id":"http-plugins/http-request-validation","title":"Request validation","description":"","sidebar":"tutorialSidebar"},"http-plugins/http-response-body-modifier":{"id":"http-plugins/http-response-body-modifier","title":"HTTP response body modifier","description":"","sidebar":"tutorialSidebar"},"http-plugins/http-response-generator":{"id":"http-plugins/http-response-generator","title":"HTTP response generator","description":"","sidebar":"tutorialSidebar"},"http-plugins/websocket-validation":{"id":"http-plugins/websocket-validation","title":"Websocket message validation","description":"","sidebar":"tutorialSidebar"},"install":{"id":"install","title":"Install","description":"Download Otoroshi","sidebar":"tutorialSidebar"},"llm-gateway/expose":{"id":"llm-gateway/expose","title":"Expose your LLM Provider","description":"now that your provider is fully setup, you can expose it to your organization. The idea here is to do it through an Otoroshi route","sidebar":"tutorialSidebar"},"llm-gateway/observability-reporting":{"id":"llm-gateway/observability-reporting","title":"Observability","description":"Every interaction with a Large Language Model (LLM) generates crucial data that can be monitored, analyzed, and optimized.","sidebar":"tutorialSidebar"},"llm-gateway/philosophy":{"id":"llm-gateway/philosophy","title":"Philosophy","description":"Today, generative AI is simply unavoidable. It\u2019s transforming industries, unlocking new possibilities, and accelerating innovation","sidebar":"tutorialSidebar"},"llm-gateway/providers":{"id":"llm-gateway/providers","title":"Supported LLM Providers","description":"Here is a list of all providers available in the Otoroshi LLM Extension :","sidebar":"tutorialSidebar"},"llm-gateway/resilience":{"id":"llm-gateway/resilience","title":"Resilience","description":"Resilience ensures that LLM interactions remain highly available and fault-tolerant, even when providers experience outages or failures.","sidebar":"tutorialSidebar"},"llm-gateway/secrets":{"id":"llm-gateway/secrets","title":"Managing secrets","description":"You can use a secret vault to save your LLM provider tokens. The main advantage here is to avoid spreading those token and giving otoroshi apikeys to your own people instead.","sidebar":"tutorialSidebar"},"llm-gateway/setup-provider":{"id":"llm-gateway/setup-provider","title":"Setup a new LLM Provider","description":"Open your Otoroshi admin web UI on otoroshi.oto.tools:8080","sidebar":"tutorialSidebar"},"overview":{"id":"overview","title":"Overview","description":"Connect, setup, secure and seamlessly manage LLM models using an Universal/OpenAI compatible API","sidebar":"tutorialSidebar"},"prompt-engineering/overview":{"id":"prompt-engineering/overview","title":"Overview","description":"Prompt engineering enhances interactions with LLMs by optimizing prompt structure, leveraging contextual information, and utilizing templating techniques.","sidebar":"tutorialSidebar"},"prompt-engineering/prompt-context":{"id":"prompt-engineering/prompt-context","title":"Prompt context","description":"","sidebar":"tutorialSidebar"},"prompt-engineering/prompt-templating":{"id":"prompt-engineering/prompt-templating","title":"Prompt templating","description":"do not forget to disable options override","sidebar":"tutorialSidebar"},"try-it":{"id":"try-it","title":"Try Otoroshi LLM Extension","description":"there is currently several ways to try the Otoroshi LLM Extension","sidebar":"tutorialSidebar"}}}')}}]);
---
sidebar_position: 6
---

import Terminal from '@site/src/components/Terminal';

# Observability & Reporting

Every interaction with a **Large Language Model (LLM)** generates crucial data that can be monitored, analyzed, and optimized. 

Our **LLM Gateway** provides **real-time tracking, security, and performance insights**, acting as a **centralized observability layer** to streamline LLM interactions.

A **gateway** is the **ultimate solution** for managing, analyzing, and securing LLM traffic. By routing all requests through our LLM gateway, you gain:

## ðŸ“Š Key Metrics We Track

Every LLM request logs **critical telemetry data**, including:

ðŸ”¹ **LLM Provider** â€“ Identify the AI service in use (e.g., OpenAI, Anthropic, Google Gemini).

ðŸ”¹ **Model Version** â€“ Track which model (GPT-4, Claude, Gemini) is processing requests.

ðŸ”¹ **Prompt Data** â€“ Log input prompts to analyze patterns & improve outputs.

ðŸ”¹ **Response Data** â€“ Capture AI-generated outputs for debugging & quality control.

ðŸ”¹ **Token Usage Metrics** â€“ Measure input/output token consumption to optimize performance.

ðŸ”¹ **User Identity** â€“ Associate API usage with specific users for accountability.

ðŸ”¹ **API Key Tracking** â€“ Monitor & secure API access to prevent unauthorized use.

ðŸ”¹ **Authentication Tokens** â€“ Ensure session integrity & compliance.

ðŸ”¹ **Connected User Sessions** â€“ Identify active users interacting with the model.

Unlike traditional monitoring tools, a **gateway** provides **full-stack observability** by capturing **every LLM request before it reaches the provider**. 

This enables **granular control, cost efficiency, and real-time insights**.
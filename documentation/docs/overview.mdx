---
sidebar_position: 1
---

import Terminal from '@site/src/components/Terminal';

# Overview

Otoroshi LLM Extension is set of Otoroshi plugins and resources to interact with LLMs, let's discover it !

**Connect, setup, secure and seamlessly manage LLM models using an Universal/OpenAI compatible API**

  - [**Unified interface**](/docs/llm-gateway/philosophy): Simplify interactions and minimize integration hassles
  - [**Use multiple providers**](/docs/llm-gateway/providers): 10+ LLM providers supported right now, a lot more coming
  - [**Load balancing**](/docs/llm-gateway/resilience): Ensure optimal performance by distributing workloads across multiple providers
  - [**Fallbacks**](docs/llm-gateway/resilience#provider-fallback): Automatically switch LLMs during failures to deliver uninterrupted & accurate performance
  - [**Automatic retries**](/docs/llm-gateway/resilience): LLM APIs often have inexplicable failures. You can rescue a substantial number of your requests with our in-built automatic retries feature.
  - [**Semantic cache**](/docs/cost-optimizations/semantic-cache): Speed up repeated queries, enhance response times, and reduce costs
  - [**Custom quotas**](/docs/cost-optimizations/quotas): Manage LLM tokens quotas per consumer and optimise costs
  - [**Key vault**](/docs/llm-gateway/secrets): securely store your LLM API keys in Otoroshi vault or any other secret vault supported by Otoroshi.
  - **[Observability](/docs/llm-gateway/observability-reporting) and [reporting](/docs/reporting/)**: every LLM request is audited with details about the consumer, the LLM provider and usage. All those audit events are exportable using multiple methods for further reporting
  - [**Fine grained authorizations**](/docs/llm-gateway/secure-provider): Use Otoroshi advanced fine grained authorizations capabilities to constrains model usage based on whatever you want: user identity, apikey, consumer metadata, request details, etc
  - [**Prompt engineering**](/docs/prompt-engineering/overview): enhance your experience by providing contextual information to your prompts, storing them in a library for reusability, and using prompt templates for increased efficiency
  - [**Guardrails**](/docs/guardrails/overview): Validate your prompts and prompts responses to avoid sensitive or personal informations leakage, irrelevant or unhelpful responses, gibberish content, etc
  - [**Cost Tracking**](/docs/cost-optimizations/cost-tracking)
  - [**MCP (Model Context protocol)**](/docs/function-calling/mcp)
  - [**MCP Connectors**](/docs/function-calling/mcp-connectors)
  - [**MCP Server exposition**](/docs/function-calling/mcp-plugins)
  - [**Wasm functions (tool calls)**](/docs/function-calling/function-wasm-calling)
  - [**Http functions (tool calls)**](/docs/function-calling/http-functions)
  - [**Embeddings models**](/docs/embeddings/overview)

## Supported LLM providers

All supported LLM providers are listed [here](/docs/llm-gateway/providers)

* Anthropic 
* Azure OpenAI
* Cloudflare
* Cohere
* Gemini
* Groq
* Huggingface ðŸ‡«ðŸ‡· ðŸ‡ªðŸ‡º
* Mistral ðŸ‡«ðŸ‡· ðŸ‡ªðŸ‡º
* Ollama (Local Models)
* OpenAI
* OVH AI Endpoints ðŸ‡«ðŸ‡· ðŸ‡ªðŸ‡º
* Scaleway ðŸ‡«ðŸ‡· ðŸ‡ªðŸ‡º
* X.ai
* Deepseek

## MCP Servers

[MCP Servers](/docs/function-calling/mcp)

## Cost Optimization

[Cost optimization](/docs/cost-optimizations)

## Ecological impacts

[Ecological impacts](/docs/reporting/ecological)

## Introduction video

<iframe width="956" height="538" src="https://www.youtube.com/embed/M8sA9xuE3gs?si=nSB80iS1Et9Q3eIT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
